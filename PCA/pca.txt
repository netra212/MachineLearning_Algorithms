# This is the feature extraction techniques. 
# Point to Remember: This is not the feature selection technique. 

# features -> dimensional. 
# f_n < f_m
# Curse of Dimensionality:
-> It says that whenever we add the more and more features, the accuracy of the model get increased but there is a point where a accuracy of the model start to decrease so we have select the optimal number of features. 

# What is problems in High Dimensional ?
-> sparsity. 

# Geometric Intuition of the PCA:
- PCA captures the higher dimensional data into lower data means try to capture the assence of the data of higher dimensional. 
- PCA is a dimensional reduction technique which can transform higher dimensional data into lower dimensional while keeping the assence of the data. 

Q. What Is Principal Component Analysis?
- Principal component analysis (PCA) is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends.

- Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.

- Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize, and thus make analyzing data points much easier and faster for machine learning algorithms without extraneous variables to process.

So, to sum up, the idea of PCA is simple: reduce the number of variables of a data set, while preserving as much information as possible.

Benefits:
1. Faster execution of Algorithm.
2. Visualization.

# Principal component analysis can be broken down into five steps.
# How we Do a Principal Component Analysis ?
    1. Standardize the range of continuous initial variables. 
    2. Compute the covariance matrix to identify correlations.
    3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components. 
    4. Create a feature vector to decide which principal components to keep.
    5. Recast the data along the principal components axes.

# What Are Principal Components ?
- Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below.

# PCA -> Forgets about the existing features and build the new complete sets of the features and PCA choose the new subset from the new set of feature which PCA thinks that particular feature is importance 
    - our datasets:
    room | washrooms | price 

    - pca build this features:
    size | price

Note:- Spread is not exactly the variance but variance is proportion to the spread. 
